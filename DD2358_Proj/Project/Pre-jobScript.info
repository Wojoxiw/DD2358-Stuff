This file describes what I need to run before/after using the jobscript.sh
Can use 'projinfo' to see project, user, and time allocation.
#commented lines should only need to be run once, to set up/install packages on the submission node. Maybe need to run it every time I log in?

First download my python scripts from github with: 

#git clone -n --depth=1 --filter=tree:0 https://github.com/Wojoxiw/DD2358-Stuff
#cd "DD2358-Stuff"
#git sparse-checkout set --no-cone "/DD2358_Proj/Project"				
#git checkout
## to update the scripts (overwriting local changes {I make none})
git fetch --all 
git reset --hard origin/master

Then load the modules that are available:

#module load GCC/13.3.0
#module load OpenMPI/5.0.3
#module load mpi4py/4.0.1
#module load SciPy-bundle/2024.05
#module load GCC/10.3.0 ##need this older version for MPICH... I guess I can just load this after the rest?? seems to unload the above lines.
#module load MPICH/3.4.2
#module save
module restore  ## just need to restore the above, hopefully

Then use Anaconda to use the installed the ones that aren't:

module load Anaconda3
source config_conda.sh
#conda create -n fenicsx-env
conda activate fenicsx-env ## hopefully this is the only command needed once stuff is installed
#conda install -c conda-forge fenics-dolfinx mpich pyvista # Linux and macOS
#conda install scipy matplotlib python-gmsh pyvista pyvistaqt spgl1 h5py petsc=*=complex* memory_profiler

Then sync output folder with my local output folder, so I have the files locally... by running the following script locally, afterward

rsync -ar --info=progress2 alepal@cosmos.lunarc.lu.se:/home/alepal/Alexandros/scatt3DTesting/DD2358-Stuff/DD2358_Proj/Project/data3D/ "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/data3DLUNARC"
rsync -ar --info=progress2 alepal@cosmos.lunarc.lu.se:/home/alepal/Alexandros/scatt3DTesting/DD2358-Stuff/DD2358_Proj/Project/prevRuns.info "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/prevRuns.info"


To run the jobscript:
sbatch jobscript.sh


Some info:
## login with ssh cosmos.lunarc.lu.se -l alepal
## or ssh alepal@cosmos.lunarc.lu.se
## COSMOS node local disk has ~1.6 TB SSD, default 5300MB RAM per core. can do #SBATCH --mem-per-cpu=10600
## variable SNIC_TMP addresses the node-local disk

#### find job with squeue -u alepal, or squeue --me
#read something? with more slurm-job#.out
##cancel job with scancel job#
## queue jobs sequentially with sbatch -d afterok:firstjobid run_script.sh

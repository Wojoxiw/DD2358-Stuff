This file describes what I need to run before/after using the jobscript.sh
Can use 'projinfo' to see project, user, and time allocation (must first deactivate mamba/conda). 'snicquota' shows storage quotas.
#Installation should only need to be run once, to set up/install packages on the submission node. It needs to be activated every log-in, though.

First download my python scripts from github with: 

git clone -n --depth=1 --filter=tree:0 https://github.com/Wojoxiw/DD2358-Stuff
cd "DD2358-Stuff"
git sparse-checkout set --no-cone "/DD2358_Proj/Project"				
git checkout
## to update the scripts (overwriting local changes {I make none})
git fetch --all 
git reset --hard origin/master

Then install the needed packages in an env:
module load foss
module save ## to make this default so it autoloads

# Actual installation (starting with deleting a previous one)  Removed installing py-pyvista on the cluster
rm -rf spack
rm -rf .spack
git clone https://github.com/spack/spack.git
. ./spack/share/spack/setup-env.sh
spack env create Scatt3D
spack env activate Scatt3D
spack add fenics-dolfinx+adios2 ^petsc+complex+mumps py-fenics-dolfinx libpressio cflags="-O3" fflags="-O3"
spack add py-gmsh py-scipy py-memory-profiler
spack compiler find
spack external find openmpi
spack install
spack env deactivate
spack env activate Scatt3D
## Must reactivate environment afterwards, or I get some libfabric 1.8 error
## May get an error on libpressio or other packages - try adding it with the above flags, then spack concretize --force, then install again

Once stuff is installed, must make sure the env is activated (and test the 'textExample.py' script for ImportErrors) before running jobscript:
. ./spack/share/spack/setup-env.sh ## when in the folder containing the spack folder
spack env activate Scatt3D
python testExample.py

To run the jobscript:
sbatch jobscript.sh

Then sync output folder with my local output folder, so I have the files locally... by running the following script locally, afterward

rsync -ar --info=progress2 alepal@cosmos.lunarc.lu.se:/home/alepal/Alexandros/scatt3D/DD2358-Stuff/DD2358_Proj/Project/data3D/ "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/data3DLUNARC"
cp -p "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/data3DLUNARC/prevRuns.npz" "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/data3D/prevRuns.npz"
## then move over the new prevRuns file for use

#rsync -ar --info=progress2 alepal@cosmos.lunarc.lu.se:/home/alepal/Alexandros/scatt3DTesting/DD2358-Stuff/DD2358_Proj/Project/prevRuns.info "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/prevRuns.info" # this is unneeded now

Some info:
## login with ssh cosmos.lunarc.lu.se -l alepal
## or ssh alepal@cosmos.lunarc.lu.se
## COSMOS node local disk has ~1.6 TB SSD, default 5300MB RAM per core. can do #SBATCH --mem-per-cpu=10600
## variable SNIC_TMP addresses the node-local disk

#### find job with squeue -u alepal, or squeue --me
#read something? with more slurm-job#.out
##cancel job with scancel job#
## queue jobs sequentially with sbatch -d afterok:firstjobid run_script.sh


##
OLD STUFF BELOW - DEPRECATED
##
Old way to install: use mamba/conda to install. This is easy and faster, but cannot use MPI across nodes.
Can install mamba from https://github.com/conda-forge/miniforge#mambaforge, otherwise just use conda

mamba create --name Scatt3D
mamba activate Scatt3D
# mamba env remove -n Scatt3D # to remove it for reinstalling. May need to module purge if MPI problems?
mamba install fenics-dolfinx mpich petsc=*=complex*
mamba install scipy matplotlib python-gmsh pyvista pyvistaqt spgl1 h5py psutil memory_profiler numpy miepython
pip install imageio[ffmpeg]
##

Potential jobscript thing to send/retrieve data to node-local disks:

## NODE LOCAL DISK
## if using node-local disk, move files over to it before running, then back after
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p runScatt3D.py $SNIC_TMP ## reads this file into the node-local disks/execution directories. I first update it with git pull origin master
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p meshMaker.py $SNIC_TMP
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p memTimeEstimation.py $SNIC_TMP
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p scatteringProblem.py $SNIC_TMP
cd $SNIC_TMP ## go to that directory to make the data 3D folder so I can move my input file(s) there... then go back to move stuff.
mkdir data3D ## Presumably (hopefully) the rank 0 process is here, so all files will be saved in this directory
cd $SLURM_SUBMIT_DIR
cp -p data3D/prevRuns.npz $SNIC_TMP"/data3D"
cd $SNIC_TMP ## go to that directory to run the script

#time python Scatt3D.py ### then run it... and time it
export MPInum=12 ## number of MPI processes
time mpirun -n $MPInum python runScatt3D.py ## run the main process, and time it
#mpirun --bind-to core python runScatt3D.py

# cp -p prevRuns.info $SLURM_SUBMIT_DIR ## copies this file out to whatever directory you were in when using sbatch jobscript.sh
cp -rp data3D $SLURM_SUBMIT_DIR/ ## copy the data folder over also
## NODE LOCAL DISK
'
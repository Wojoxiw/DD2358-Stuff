This file describes what I need to run before/after using the jobscript.sh
Can use 'projinfo' to see project, user, and time allocation (must first deactivate mamba/conda).
#commented lines should only need to be run once, to set up/install packages on the submission node. Maybe need to run it every time I log in?

First download my python scripts from github with: 

#git clone -n --depth=1 --filter=tree:0 https://github.com/Wojoxiw/DD2358-Stuff
#cd "DD2358-Stuff"
#git sparse-checkout set --no-cone "/DD2358_Proj/Project"				
#git checkout
## to update the scripts (overwriting local changes {I make none})
git fetch --all 
git reset --hard origin/master

Then install the needed packages in an env:
Loading the modules that are available always seems to give an error such as ModuleNotFoundError: No module named 'numpy', so just install all with mamba
Can install mamba from https://github.com/conda-forge/miniforge#mambaforge, otherwise just use conda

mamba create --name Scatt3D
mamba activate Scatt3D
# mamba env remove -n Scatt3D # to remove it for reinstalling. May need to module purge if MPI problems?
mamba install fenics-dolfinx mpich petsc=*=complex*
mamba install scipy matplotlib python-gmsh pyvista pyvistaqt spgl1 h5py psutil memory_profiler numpy
pip install imageio[ffmpeg]

Once stuff is installed, must make sure mamba is activated (and test the script for ImportErrors) before running jobscript:

To run the jobscript:
sbatch jobscript.sh

Then sync output folder with my local output folder, so I have the files locally... by running the following script locally, afterward

rsync -ar --info=progress2 alepal@cosmos.lunarc.lu.se:/home/alepal/Alexandros/scatt3DTesting/DD2358-Stuff/DD2358_Proj/Project/data3D/ "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/data3DLUNARC"
 #rsync -ar --info=progress2 alepal@cosmos.lunarc.lu.se:/home/alepal/Alexandros/scatt3DTesting/DD2358-Stuff/DD2358_Proj/Project/prevRuns.info "/mnt/d/Microwave Imaging/repository/DD2358_Proj/Project/prevRuns.info" # this is unneeded now

Some info:
## login with ssh cosmos.lunarc.lu.se -l alepal
## or ssh alepal@cosmos.lunarc.lu.se
## COSMOS node local disk has ~1.6 TB SSD, default 5300MB RAM per core. can do #SBATCH --mem-per-cpu=10600
## variable SNIC_TMP addresses the node-local disk

#### find job with squeue -u alepal, or squeue --me
#read something? with more slurm-job#.out
##cancel job with scancel job#
## queue jobs sequentially with sbatch -d afterok:firstjobid run_script.sh




Potential jobscript thing to send/retrieve data to node-local disks:
: '
## NODE LOCAL DISK
## if using node-local disk, move files over to it before running, then back after
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p runScatt3D.py $SNIC_TMP ## reads this file into the node-local disks/execution directories. I first update it with git pull origin master
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p meshMaker.py $SNIC_TMP
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p memTimeEstimation.py $SNIC_TMP
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p scatteringProblem.py $SNIC_TMP
cd $SNIC_TMP ## go to that directory to make the data 3D folder so I can move my input file(s) there... then go back to move stuff.
mkdir data3D ## Presumably (hopefully) the rank 0 process is here, so all files will be saved in this directory
cd $SLURM_SUBMIT_DIR
cp -p data3D/prevRuns.npz $SNIC_TMP"/data3D"
cd $SNIC_TMP ## go to that directory to run the script

#time python Scatt3D.py ### then run it... and time it
export MPInum=12 ## number of MPI processes
time mpirun -n $MPInum python runScatt3D.py ## run the main process, and time it
#mpirun --bind-to core python runScatt3D.py

# cp -p prevRuns.info $SLURM_SUBMIT_DIR ## copies this file out to whatever directory you were in when using sbatch jobscript.sh
cp -rp data3D $SLURM_SUBMIT_DIR/ ## copy the data folder over also
## NODE LOCAL DISK
'